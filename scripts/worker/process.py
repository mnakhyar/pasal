"""Process pending crawl jobs: download PDF, parse, load to Supabase.

Picks up pending jobs from crawl_jobs table, downloads the PDF,
parses it into structured nodes, and loads into Supabase.

PDF tracking: stores SHA-256 hash, size, and download timestamp.
If a PDF already exists locally with the same hash, skips re-download.
"""
import asyncio
import hashlib
import re
import ssl
import sys
import time
from datetime import datetime, timezone
from pathlib import Path

import httpx
from bs4 import BeautifulSoup

sys.path.insert(0, str(Path(__file__).parent.parent))
from crawler.config import DEFAULT_HEADERS, DELAY_BETWEEN_REQUESTS
from crawler.db import get_sb
from crawler.state import claim_pending_jobs, update_status
from loader.load_to_supabase import (
    cleanup_work_data,
    create_chunks,
    init_supabase,
    load_nodes_recursive,
    load_work,
)
from parser.extract_pymupdf import extract_text_pymupdf
from parser.classify_pdf import classify_pdf_quality
from parser.ocr_correct import correct_ocr_errors
from parser.parse_structure import parse_structure

PDF_DIR = Path(__file__).parent.parent.parent / "data" / "raw" / "pdfs"
STORAGE_BUCKET = "regulation-pdfs"

# Bump this when the parser changes significantly to trigger re-extraction.
# v1: original parser (sort_order * 100 per level — overflows bigint)
# v2: DFS counter sort_order (1, 2, 3, …) — no overflow possible
# v3: text-first parser — captures all text, preambles, OCR corrections
# v4: OCR on all PDFs, Roman Pasal fix, FRESIDEN/header stripping, PENJELASAN fallback
EXTRACTION_VERSION = 4


async def _extract_pdf_url_from_detail_page(
    client: httpx.AsyncClient, detail_url: str
) -> tuple[str | None, str | None]:
    """Fetch a regulation detail page and extract the real PDF URL.

    peraturan.go.id uses unpredictable PDF filenames (e.g. ps4-2022.pdf
    for perpres-no-4-tahun-2022), so we must scrape the detail page
    to find the actual download link.

    Returns (pdf_url, error_reason) — error_reason is set when extraction fails.
    """
    try:
        resp = await client.get(detail_url, headers={
            **DEFAULT_HEADERS,
            "Accept": "text/html,application/xhtml+xml,*/*",
        })
        if resp.status_code != 200:
            return None, f"HTTP {resp.status_code}"

        soup = BeautifulSoup(resp.text, "html.parser")

        # Strategy 1: look for "Dokumen Peraturan" row in metadata table
        for row in soup.find_all("tr"):
            th = row.find("th")
            td = row.find("td")
            if not th or not td:
                continue
            key = th.get_text(strip=True).lower()
            if "dokumen" in key:
                pdf_link = td.find("a", href=re.compile(r"\.pdf", re.IGNORECASE))
                if pdf_link:
                    href = pdf_link["href"]
                    url = href if href.startswith("http") else f"https://peraturan.go.id{href}"
                    return url, None

        # Strategy 2: any <a> tag with .pdf or /files/ in href
        for link in soup.find_all("a", href=True):
            href = link["href"]
            if href.endswith(".pdf") or "/files/" in href:
                url = href if href.startswith("http") else f"https://peraturan.go.id{href}"
                return url, None

        return None, "page loaded but no PDF link in HTML"

    except Exception as e:
        return None, f"network error: {e}"


def _upload_to_storage(db, slug: str, pdf_bytes: bytes) -> str | None:
    """Upload PDF to Supabase Storage. Returns public URL or None on failure."""
    storage_path = f"{slug}.pdf"
    try:
        db.storage.from_(STORAGE_BUCKET).upload(
            storage_path,
            pdf_bytes,
            {"content-type": "application/pdf", "upsert": "true"},
        )
        url = db.storage.from_(STORAGE_BUCKET).get_public_url(storage_path)
        return url
    except Exception as e:
        # Don't fail the whole job if storage upload fails
        print(f"    Storage upload failed: {e}")
        return None


def _storage_exists(db, slug: str) -> str | None:
    """Check if PDF already exists in Supabase Storage. Returns public URL or None."""
    storage_path = f"{slug}.pdf"
    try:
        # Try to get file info — will raise if not found
        db.storage.from_(STORAGE_BUCKET).list(path="", options={"search": storage_path, "limit": 1})
        url = db.storage.from_(STORAGE_BUCKET).get_public_url(storage_path)
        return url
    except Exception:
        return None


def _sha256(path: Path) -> str:
    """Compute SHA-256 hex digest of a file."""
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def _create_run(source_id: str | None) -> int:
    """Create a scraper_runs record and return its ID."""
    sb = get_sb()
    result = sb.table("scraper_runs").insert({
        "source_id": source_id or "all",
        "status": "running",
        "started_at": datetime.now(timezone.utc).isoformat(),
    }).execute()
    return result.data[0]["id"]


def _update_run(run_id: int, stats: dict, status: str = "completed", error: str | None = None) -> None:
    """Update a scraper_runs record with final stats."""
    sb = get_sb()
    update = {
        "status": status,
        "jobs_processed": stats.get("processed", 0),
        "jobs_succeeded": stats.get("succeeded", 0),
        "jobs_failed": stats.get("failed", 0),
        "completed_at": datetime.now(timezone.utc).isoformat(),
    }
    if error:
        update["error_message"] = error
    sb.table("scraper_runs").update(update).eq("id", run_id).execute()


def _build_law_dict(job: dict, text: str, nodes: list) -> dict:
    """Build the law dict expected by load_to_supabase from job metadata."""
    reg_type = job.get("regulation_type", "UU")
    number = job.get("number", "")
    year = job.get("year", 0)
    title = job.get("title", f"{reg_type} {number}/{year}")
    frbr_uri = job.get("frbr_uri", f"/akn/id/act/{reg_type.lower()}/{year}/{number}")

    return {
        "frbr_uri": frbr_uri,
        "type": reg_type,
        "number": number,
        "year": year,
        "title_id": title,
        "status": "berlaku",
        "source_url": job.get("url"),
        "source_pdf_url": job.get("pdf_url"),
        "full_text": text,
        "nodes": nodes,
    }


def _extract_and_load(sb, job: dict, pdf_path: Path) -> tuple[int, int, int]:
    """Extract text from PDF, parse, and load to Supabase.

    Uses the text-first parser pipeline: extract → classify → OCR correct → parse.
    Returns (work_id, pasal_count, chunk_count).
    Raises on failure.
    """
    text, _ = extract_text_pymupdf(pdf_path)
    if not text or len(text) < 100:
        raise ValueError(f"PDF text too short ({len(text)} chars)")

    quality, _ = classify_pdf_quality(pdf_path)
    # OCR correction for all PDFs — even born_digital has font-encoding artifacts
    text = correct_ocr_errors(text)

    nodes = parse_structure(text)
    law = _build_law_dict(job, text, nodes)

    work_id = load_work(sb, law)
    if not work_id:
        raise ValueError("Failed to upsert work")

    cleanup_work_data(sb, work_id)
    pasal_nodes = load_nodes_recursive(sb, work_id, nodes)
    chunk_count = create_chunks(sb, work_id, law, pasal_nodes)

    return work_id, len(pasal_nodes), chunk_count


async def process_jobs(
    source_id: str | None = None,
    batch_size: int = 20,
    max_runtime: int = 1500,
    run_id: int | None = None,
) -> dict:
    """Process pending crawl_jobs through the full pipeline.

    Uses atomic claim_pending_jobs() so multiple workers never get the same jobs.
    Downloads PDF (or uses cached copy), parses, loads to Supabase.
    Tracks PDF hash, size, and download timestamp for reproducibility.
    """
    stats = {"processed": 0, "succeeded": 0, "failed": 0, "skipped": 0}
    start_time = time.time()

    jobs = claim_pending_jobs(limit=batch_size)
    if not jobs:
        print("  No pending jobs found")
        return stats

    print(f"  Found {len(jobs)} pending jobs")

    sb = init_supabase()
    db = get_sb()

    ctx = ssl.create_default_context()
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    transport = httpx.AsyncHTTPTransport(retries=3, verify=ctx)

    async with httpx.AsyncClient(timeout=60, transport=transport, follow_redirects=True) as client:
        for job in jobs:
            elapsed = time.time() - start_time
            if elapsed > max_runtime:
                print(f"  Runtime limit reached ({max_runtime}s), stopping")
                break

            job_id = job["id"]
            stored_pdf_url = job.get("pdf_url")
            slug = job.get("url", "").split("/")[-1] or f"job_{job_id}"
            detail_url = job.get("url", f"https://peraturan.go.id/id/{slug}")

            print(f"\n  [{stats['processed']+1}/{len(jobs)}] Processing {slug}...")

            try:
                # Link to run
                if run_id:
                    db.table("crawl_jobs").update({"run_id": run_id}).eq("id", job_id).execute()

                pdf_path = PDF_DIR / f"{slug}.pdf"
                now = datetime.now(timezone.utc).isoformat()

                # 1. Download PDF (or use cached copy)
                if pdf_path.exists() and pdf_path.stat().st_size >= 1000:
                    local_hash = _sha256(pdf_path)
                    existing_hash = job.get("pdf_hash")
                    if existing_hash and existing_hash == local_hash:
                        print(f"    Using cached PDF (hash match: {local_hash[:12]}...)")
                    else:
                        print(f"    PDF exists locally, computing hash...")
                else:
                    pdf_path.parent.mkdir(parents=True, exist_ok=True)

                    # Resolve the real PDF URL from the detail page.
                    # peraturan.go.id uses unpredictable PDF filenames (e.g. ps4-2022.pdf
                    # for perpres-no-4-tahun-2022), so guessing from slugs doesn't work.
                    pdf_url = None
                    tried_urls: list[str] = []
                    attempt_errors: list[str] = []

                    print(f"    Fetching detail page: {detail_url}")
                    real_pdf_url, extract_err = await _extract_pdf_url_from_detail_page(client, detail_url)
                    if real_pdf_url:
                        print(f"    PDF URL from detail page: {real_pdf_url}")
                        pdf_url = real_pdf_url
                    else:
                        msg = f"detail_page({detail_url}): {extract_err}"
                        print(f"    {msg}")
                        attempt_errors.append(msg)

                    # Build candidate list: detail page URL first, then stored URL as fallback
                    candidates = []
                    if pdf_url:
                        candidates.append(pdf_url)
                    if stored_pdf_url and stored_pdf_url not in candidates:
                        candidates.append(stored_pdf_url)

                    if not candidates:
                        raise ValueError(
                            f"No PDF URL found | detail_page: {detail_url} | stored: {stored_pdf_url}"
                        )

                    # Rate-limit: pause before downloading after fetching the detail page
                    await asyncio.sleep(DELAY_BETWEEN_REQUESTS)

                    pdf_content: bytes | None = None
                    for attempt_url in candidates:
                        tried_urls.append(attempt_url)
                        try:
                            resp = await client.get(attempt_url, headers=DEFAULT_HEADERS)
                            resp.raise_for_status()
                            content_type = resp.headers.get("content-type", "")
                            if "pdf" not in content_type and "octet-stream" not in content_type:
                                msg = f"{attempt_url}: not a PDF (content-type: {content_type})"
                                print(f"    {msg}")
                                attempt_errors.append(msg)
                                continue
                            if len(resp.content) < 1000:
                                msg = f"{attempt_url}: too small ({len(resp.content)} bytes)"
                                print(f"    {msg}")
                                attempt_errors.append(msg)
                                continue
                            pdf_content = resp.content
                            pdf_url = attempt_url
                            break
                        except httpx.HTTPStatusError as e:
                            msg = f"{attempt_url}: HTTP {e.response.status_code}"
                            print(f"    {msg}")
                            attempt_errors.append(msg)
                            continue

                    if pdf_content is None:
                        raise ValueError(
                            f"PDF download failed | tried: {tried_urls} | errors: {attempt_errors}"
                        )

                    pdf_path.write_bytes(pdf_content)
                    local_hash = _sha256(pdf_path)
                    print(f"    Downloaded {pdf_path.stat().st_size:,} bytes from {pdf_url}")

                    # Persist the confirmed working PDF URL + attempt log
                    db.table("crawl_jobs").update({
                        "pdf_url": pdf_url,
                        "updated_at": now,
                    }).eq("id", job_id).execute()

                # Store PDF metadata
                local_hash = _sha256(pdf_path)
                pdf_size = pdf_path.stat().st_size
                db.table("crawl_jobs").update({
                    "status": "downloaded",
                    "pdf_hash": local_hash,
                    "pdf_size": pdf_size,
                    "pdf_downloaded_at": now,
                    "pdf_local_path": str(pdf_path),
                    "updated_at": now,
                }).eq("id", job_id).execute()

                # 1b. Upload PDF to Supabase Storage for persistence
                storage_url = _upload_to_storage(db, slug, pdf_path.read_bytes())
                if storage_url:
                    print(f"    Uploaded to storage: {slug}.pdf")

                # 2. Extract, parse, load
                work_id, pasal_count, chunk_count = _extract_and_load(sb, job, pdf_path)

                # 3. Mark as loaded with extraction version + storage URL
                loaded_update: dict = {
                    "status": "loaded",
                    "work_id": work_id,
                    "extraction_version": EXTRACTION_VERSION,
                    "updated_at": datetime.now(timezone.utc).isoformat(),
                }
                if storage_url:
                    loaded_update["pdf_storage_url"] = storage_url
                db.table("crawl_jobs").update(loaded_update).eq("id", job_id).execute()

                stats["succeeded"] += 1
                print(f"    OK: {pasal_count} pasals, {chunk_count} chunks, hash={local_hash[:12]}...")

            except httpx.HTTPStatusError as e:
                error_msg = f"HTTP {e.response.status_code}"
                update_status(job_id, "failed", error_msg)
                stats["failed"] += 1
                print(f"    FAIL: {error_msg}")

            except Exception as e:
                update_status(job_id, "failed", str(e)[:1000])
                stats["failed"] += 1
                print(f"    FAIL: {e}")

            stats["processed"] += 1
            await asyncio.sleep(DELAY_BETWEEN_REQUESTS)

    return stats


def _download_from_storage(db, slug: str, dest: Path) -> bool:
    """Download PDF from Supabase Storage to local path. Returns True on success."""
    try:
        data = db.storage.from_(STORAGE_BUCKET).download(f"{slug}.pdf")
        if data and len(data) > 1000:
            dest.parent.mkdir(parents=True, exist_ok=True)
            dest.write_bytes(data)
            return True
    except Exception as e:
        print(f"    Storage download failed: {e}")
    return False


def reprocess_jobs(
    batch_size: int = 50,
    force: bool = False,
) -> dict:
    """Re-extract and reload from PDFs (local cache or Supabase Storage).

    Finds jobs that are 'loaded' or 'parsed' and have an outdated
    extraction_version, then re-runs the extraction pipeline.
    Downloads PDFs from Supabase Storage when local files are missing
    (e.g. after Railway container restart).

    Args:
        batch_size: Max jobs to reprocess.
        force: If True, reprocess all loaded jobs. If False, only reprocess
               jobs with extraction_version < EXTRACTION_VERSION.
    """
    stats = {"processed": 0, "succeeded": 0, "failed": 0, "skipped": 0}
    db = get_sb()
    sb = init_supabase()

    # Find loaded jobs needing re-extraction
    query = db.table("crawl_jobs").select("*").in_("status", ["loaded", "parsed", "downloaded"])
    if not force:
        # Only reprocess if extraction version is outdated
        query = query.lt("extraction_version", EXTRACTION_VERSION)
    result = query.limit(batch_size).execute()
    jobs = result.data or []

    if not jobs:
        print("  No jobs to reprocess")
        return stats

    print(f"  Found {len(jobs)} jobs to reprocess (extraction v{EXTRACTION_VERSION})")

    for job in jobs:
        job_id = job["id"]
        slug = job.get("url", "").split("/")[-1] or f"job_{job_id}"
        pdf_local = job.get("pdf_local_path")

        # Try to find the PDF: local cache first, then Supabase Storage
        if pdf_local:
            pdf_path = Path(pdf_local)
        else:
            pdf_path = PDF_DIR / f"{slug}.pdf"

        if not pdf_path.exists():
            print(f"  [{stats['processed']+1}/{len(jobs)}] {slug}: downloading from storage...")
            if not _download_from_storage(db, slug, pdf_path):
                print(f"    PDF not in storage either, skipping")
                stats["skipped"] += 1
                stats["processed"] += 1
                continue

        print(f"  [{stats['processed']+1}/{len(jobs)}] Reprocessing {slug}...")

        try:
            # Verify hash if available
            stored_hash = job.get("pdf_hash")
            current_hash = _sha256(pdf_path)
            if stored_hash and stored_hash != current_hash:
                print(f"    WARNING: PDF hash changed! stored={stored_hash[:12]} current={current_hash[:12]}")

            work_id, pasal_count, chunk_count = _extract_and_load(sb, job, pdf_path)

            # Update job
            db.table("crawl_jobs").update({
                "status": "loaded",
                "work_id": work_id,
                "extraction_version": EXTRACTION_VERSION,
                "pdf_hash": current_hash,
                "pdf_size": pdf_path.stat().st_size,
                "updated_at": datetime.now(timezone.utc).isoformat(),
            }).eq("id", job_id).execute()

            stats["succeeded"] += 1
            print(f"    OK: {pasal_count} pasals, {chunk_count} chunks")

        except Exception as e:
            update_status(job_id, "failed", f"reprocess: {str(e)[:400]}")
            stats["failed"] += 1
            print(f"    FAIL: {e}")

        stats["processed"] += 1

    return stats
